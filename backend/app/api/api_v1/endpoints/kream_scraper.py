"""
KREAM í¬ë¡¤ëŸ¬ API ì—”ë“œí¬ì¸íŠ¸
"""

import logging
from typing import List
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from pydantic import BaseModel

from app.api import deps
from app.models.user import User
from app.models.product import Product
from app.models.brand import Brand
from app.services.kream_scraper import KreamScraper
from app.services.kream_api_scraper import KreamAPIScraper
from app.db.database import SessionLocal

logger = logging.getLogger(__name__)

router = APIRouter()


class ScrapeRequest(BaseModel):
    keyword: str
    max_products: int = 100  # API ìµœëŒ€ì¹˜
    page: int = 1
    save_to_db: bool = True


class BulkSaveRequest(BaseModel):
    keyword: str
    max_products: int = 100
    total_pages: int = 1


class ScrapeResult(BaseModel):
    total_scraped: int
    total_saved: int
    products: List[dict]
    errors: List[str]


async def save_scraped_products_to_db(products: List[dict], current_user: User):
    """í¬ë¡¤ë§í•œ ìƒí’ˆì„ DBì— ì €ì¥"""
    db = SessionLocal()
    saved_count = 0
    errors = []

    try:
        for product_data in products:
            try:
                # ëª¨ë¸ë²ˆí˜¸ë¡œ ì¤‘ë³µ ì²´í¬
                model_number = product_data.get('model_number', '')
                if not model_number:
                    errors.append(f"ëª¨ë¸ë²ˆí˜¸ ì—†ìŒ: {product_data.get('product_name_ko', 'Unknown')}")
                    continue

                existing = db.query(Product).filter(
                    Product.product_code == model_number
                ).first()

                if existing:
                    logger.info(f"Product already exists: {model_number}")
                    errors.append(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìƒí’ˆ: {model_number}")
                    continue

                # ë¸Œëœë“œ ì°¾ê¸° ë˜ëŠ” ìƒì„±
                brand_name = product_data.get('brand', 'Unknown')
                brand = db.query(Brand).filter(Brand.name == brand_name).first()
                if not brand:
                    brand = Brand(name=brand_name, description=f'From KREAM scraping')
                    db.add(brand)
                    db.flush()  # brand.id ìƒì„±
                
                # ì„¤ëª… í•„ë“œì— ì¶”ê°€ ì •ë³´ í¬í•¨
                description_parts = []
                if product_data.get('product_name_en'):
                    description_parts.append(f"ì˜ë¬¸ëª…: {product_data['product_name_en']}")
                if product_data.get('color'):
                    description_parts.append(f"ìƒ‰ìƒ: {product_data['color']}")
                if product_data.get('release_price'):
                    description_parts.append(f"ë°œë§¤ê°€: {product_data['release_price']:,}ì›")
                if product_data.get('source_url'):
                    description_parts.append(f"ì¶œì²˜: {product_data['source_url']}")
                
                description = "\n".join(description_parts) if description_parts else None

                # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (KREAM ì¹´í…Œê³ ë¦¬ â†’ ì‹œìŠ¤í…œ ì¹´í…Œê³ ë¦¬)
                category_1d = product_data.get('category_1d', '')
                category = 'etc'  # ê¸°ë³¸ê°’

                if 'ì‹ ë°œ' in category_1d or 'ìŠ¤ë‹ˆì»¤ì¦ˆ' in category_1d:
                    category = 'shoe'
                elif 'ì•„ìš°í„°' in category_1d or 'ìƒì˜' in category_1d or 'í•˜ì˜' in category_1d or 'íŒ¨ë”©' in category_1d:
                    category = 'clothing'
                elif 'ê°€ë°©' in category_1d or 'ì§€ê°‘' in category_1d:
                    category = 'bag'
                elif 'ì•¡ì„¸ì„œë¦¬' in category_1d or 'ì‹œê³„' in category_1d or 'ëª¨ì' in category_1d:
                    category = 'accessory'

                logger.info(f"   ì¹´í…Œê³ ë¦¬ ë§¤í•‘: '{category_1d}' â†’ '{category}'")

                # ìƒˆ ìƒí’ˆ ìƒì„±
                new_product = Product(
                    brand_id=brand.id,
                    product_code=model_number,
                    product_name=product_data.get('product_name_ko') or product_data.get('product_name_en', ''),
                    description=description,
                    image_url=product_data.get('image_url'),
                    category=category,
                )

                db.add(new_product)
                saved_count += 1

            except Exception as e:
                error_msg = f"ìƒí’ˆ ì €ì¥ ì‹¤íŒ¨ ({product_data.get('product_name_ko', 'Unknown')}): {str(e)}"
                logger.error(error_msg)
                errors.append(error_msg)

        db.commit()
        logger.info(f"Saved {saved_count} products to database")

    except Exception as e:
        db.rollback()
        logger.error(f"Database error: {e}")
        errors.append(f"DB ì˜¤ë¥˜: {str(e)}")
    finally:
        db.close()

    return saved_count, errors


@router.post("/scrape", response_model=ScrapeResult)
async def scrape_kream_products(
    request: ScrapeRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(deps.get_current_active_user),
):
    """
    KREAMì—ì„œ ìƒí’ˆì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    **ì£¼ì˜ì‚¬í•­:**
    - ì´ ê¸°ëŠ¥ì€ êµìœ¡/ê°œë°œ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”
    - ê³¼ë„í•œ ìš”ì²­ì€ IP ì°¨ë‹¨ì˜ ì›ì¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - KREAMì˜ ì´ìš©ì•½ê´€ì„ ì¤€ìˆ˜í•˜ì„¸ìš”
    """
    try:
        logger.info(f"Starting KREAM scraping: keyword={request.keyword}, max={request.max_products}")

        # í¬ë¡¤ë§ ì‹¤í–‰ - API ì§ì ‘ í˜¸ì¶œ ë°©ì‹ (ë´‡ ì°¨ë‹¨ ìš°íšŒ)
        api_scraper = KreamAPIScraper()
        products = await api_scraper.search_products(
            keyword=request.keyword,
            max_products=request.max_products,
            page=request.page
        )

        if not products:
            return ScrapeResult(
                total_scraped=0,
                total_saved=0,
                products=[],
                errors=["í¬ë¡¤ë§ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤."]
            )

        # í’ˆë²ˆ ì¤‘ë³µ ì²˜ë¦¬ ë° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        expanded_products = []
        for product_data in products:
            model_numbers = product_data.get('model_number', '').split('/')

            # í’ˆë²ˆì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ê°ê° ë³„ë„ ìƒí’ˆìœ¼ë¡œ ìƒì„±
            for model_num in model_numbers:
                model_num = model_num.strip()
                if not model_num:
                    continue

                # ìƒí’ˆ ë°ì´í„° ë³µì‚¬
                new_product = product_data.copy()
                new_product['model_number'] = model_num

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„: {new_product.get('product_name_ko', 'Unknown')} ({model_num})")
                print(f"   ì´ë¯¸ì§€ URL: {new_product.get('image_url', 'None')}")
                print(f"   ë¸Œëœë“œ: {new_product.get('brand', 'None')}")

                if new_product.get('image_url') and new_product.get('brand'):
                    try:
                        local_path = await api_scraper.download_image(
                            image_url=new_product['image_url'],
                            brand_name=new_product['brand'],
                            model_number=model_num
                        )
                        new_product['image_url'] = local_path  # ë¡œì»¬ ê²½ë¡œë¡œ êµì²´
                        print(f"   âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {local_path}")
                        logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {new_product.get('product_name_ko', 'Unknown')} ({model_num}) -> {local_path}")
                    except Exception as e:
                        print(f"   âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        logger.error(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        import traceback
                        traceback.print_exc()
                else:
                    print(f"   âš ï¸ ì´ë¯¸ì§€ URL ë˜ëŠ” ë¸Œëœë“œ ì •ë³´ ì—†ìŒ")

                expanded_products.append(new_product)

        logger.info(f"ğŸ“¦ í’ˆë²ˆ ë¶„í•  í›„ ì´ ìƒí’ˆ ìˆ˜: {len(expanded_products)}ê°œ")

        # DB ì €ì¥
        saved_count = 0
        errors = []

        if request.save_to_db:
            saved_count, errors = await save_scraped_products_to_db(expanded_products, current_user)

        return ScrapeResult(
            total_scraped=len(expanded_products),
            total_saved=saved_count,
            products=expanded_products,
            errors=errors
        )

    except Exception as e:
        logger.error(f"Scraping error: {e}")
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")


@router.post("/bulk-save", response_model=ScrapeResult)
async def bulk_save_kream_products(
    request: BulkSaveRequest,
    current_user: User = Depends(deps.get_current_active_user),
):
    """
    ì—¬ëŸ¬ í˜ì´ì§€ì˜ ìƒí’ˆì„ í•œ ë²ˆì— ìˆ˜ì§‘í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    try:
        logger.info(f"Starting bulk save: keyword={request.keyword}, pages={request.total_pages}")

        all_products = []
        api_scraper = KreamAPIScraper()

        # ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘
        for page in range(1, request.total_pages + 1):
            logger.info(f"Fetching page {page}/{request.total_pages}")
            products = await api_scraper.search_products(
                keyword=request.keyword,
                max_products=request.max_products,
                page=page
            )

            if not products:
                logger.info(f"No products found on page {page}")
                break

            # í’ˆë²ˆ ì¤‘ë³µ ì²˜ë¦¬ ë° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            for product_data in products:
                model_numbers = product_data.get('model_number', '').split('/')

                for model_num in model_numbers:
                    model_num = model_num.strip()
                    if not model_num:
                        continue

                    new_product = product_data.copy()
                    new_product['model_number'] = model_num

                    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    if new_product.get('image_url') and new_product.get('brand'):
                        try:
                            local_path = await api_scraper.download_image(
                                image_url=new_product['image_url'],
                                brand_name=new_product['brand'],
                                model_number=model_num
                            )
                            new_product['image_url'] = local_path
                            logger.info(f"âœ… Image downloaded: {model_num}")
                        except Exception as e:
                            logger.error(f"âŒ Image download failed: {e}")

                    all_products.append(new_product)

        logger.info(f"ğŸ“¦ Total products collected: {len(all_products)}")

        # DB ì €ì¥
        saved_count = 0
        errors = []

        if all_products:
            saved_count, errors = await save_scraped_products_to_db(all_products, current_user)

        return ScrapeResult(
            total_scraped=len(all_products),
            total_saved=saved_count,
            products=all_products,
            errors=errors
        )

    except Exception as e:
        logger.error(f"Bulk save error: {e}")
        raise HTTPException(status_code=500, detail=f"ì¼ê´„ ì €ì¥ ì˜¤ë¥˜: {str(e)}")


@router.get("/test")
async def test_scraper(
    keyword: str = "ë‚˜ì´í‚¤",
    max_products: int = 3,
    kream_email: str = "",
    kream_password: str = "",
    current_user: User = Depends(deps.get_current_active_user),
):
    """
    í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ (DB ì €ì¥ ì•ˆí•¨)
    """
    try:
        if not kream_email or not kream_password:
            raise HTTPException(status_code=400, detail="KREAM ê³„ì • ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        scraper = KreamScraper(
            email=kream_email,
            password=kream_password,
            headless=True
        )
        products = await scraper.scrape_products(
            keyword=keyword,
            max_products=max_products
        )

        return {
            "success": True,
            "total": len(products),
            "products": products
        }

    except Exception as e:
        logger.error(f"Test scraping error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
