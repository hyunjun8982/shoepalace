"""
KREAM ìƒí’ˆ í¬ë¡¤ëŸ¬ - Playwright ê¸°ë°˜
ì£¼ì˜: ì´ ì½”ë“œëŠ” êµìœ¡/ê°œë°œ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
ìƒì—…ì  ì‚¬ìš© ì‹œ ë²•ì  ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import asyncio
import re
import json
import logging
import os
import httpx
import time
import random
from pathlib import Path
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page, Browser, BrowserContext
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class KreamScraper:
    """KREAM í¬ë¡¤ëŸ¬ - Playwright ê¸°ë°˜ (ë¡œê·¸ì¸ ë¶ˆí•„ìš”)"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None

    async def _ensure_chromium_installed(self):
        """Chromiumì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ì„¤ì¹˜"""
        import os
        import subprocess

        chromium_path = '/root/.cache/ms-playwright/chromium-1091/chrome-linux/chrome'

        if not os.path.exists(chromium_path):
            logger.info("âš ï¸ Chromiumì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
            try:
                # Chromium ì„¤ì¹˜
                subprocess.run(
                    ['python', '-m', 'playwright', 'install', 'chromium'],
                    check=True,
                    capture_output=True,
                    text=True
                )
                logger.info("âœ… Chromium ì„¤ì¹˜ ì™„ë£Œ")
            except subprocess.CalledProcessError as e:
                logger.error(f"âŒ Chromium ì„¤ì¹˜ ì‹¤íŒ¨: {e.stderr}")
                raise

    async def _init_browser(self):
        """Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        if self.browser:
            return

        # Chromium ì„¤ì¹˜ í™•ì¸
        await self._ensure_chromium_installed()

        logger.info("Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...")
        playwright = await async_playwright().start()

        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=IsolateOrigins,site-per-process',
            ]
        )

        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
            locale='ko-KR',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
            }
        )

        # ê°•í™”ëœ ë´‡ ê°ì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
        await self.context.add_init_script("""
            // navigator.webdriver ì œê±°
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });

            // Chrome ê°ì²´ ì¶”ê°€ (HeadlessChrome ê°ì§€ ìš°íšŒ)
            window.chrome = {
                runtime: {}
            };

            // Permissions API ì˜¤ë²„ë¼ì´ë“œ
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );

            // Plugin ë°°ì—´ ì¶”ê°€
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });

            // Languages ì¶”ê°€
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en']
            });
        """)

        self.page = await self.context.new_page()
        logger.info("âœ… Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")

    async def login(self) -> bool:
        """KREAMì— ë¡œê·¸ì¸"""
        try:
            await self._init_browser()

            logger.info("KREAM ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            await self.page.goto("https://kream.co.kr/login", wait_until='domcontentloaded', timeout=60000)
            await asyncio.sleep(2)

            # ì´ë©”ì¼ ì…ë ¥
            logger.info("ì´ë©”ì¼ ì…ë ¥ ì¤‘...")
            email_selector = "input[type='email'], input[name='email'], input[placeholder*='ì´ë©”ì¼']"
            try:
                await self.page.wait_for_selector(email_selector, timeout=10000)
                await self.page.fill(email_selector, self.email)
                logger.info("âœ… ì´ë©”ì¼ ì…ë ¥ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì´ë©”ì¼ ì…ë ¥ ì‹¤íŒ¨: {e}")
                await self.page.screenshot(path='/tmp/kream_login_email_fail.png')
                return False

            # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
            logger.info("ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì¤‘...")
            password_selector = "input[type='password'], input[name='password']"
            try:
                await self.page.fill(password_selector, self.password)
                logger.info("âœ… ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì‹¤íŒ¨: {e}")
                await self.page.screenshot(path='/tmp/kream_login_password_fail.png')
                return False

            # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
            logger.info("ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì¤‘...")
            login_button_selector = "button[type='submit'], button.login_btn_box, a.login_btn_box"
            try:
                await self.page.click(login_button_selector)
                logger.info("âœ… ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
                await self.page.screenshot(path='/tmp/kream_login_button_fail.png')
                return False

            # ë¡œê·¸ì¸ ì„±ê³µ ëŒ€ê¸° (URL ë³€ê²½ í™•ì¸)
            await asyncio.sleep(5)

            current_url = self.page.url
            if "login" not in current_url:
                logger.info(f"âœ… KREAM ë¡œê·¸ì¸ ì„±ê³µ! í˜„ì¬ URL: {current_url}")
                return True
            else:
                logger.error(f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨ - ì—¬ì „íˆ ë¡œê·¸ì¸ í˜ì´ì§€: {current_url}")
                await self.page.screenshot(path='/tmp/kream_login_final_fail.png')
                return False

        except Exception as e:
            logger.error(f"âŒ ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            if self.page:
                await self.page.screenshot(path='/tmp/kream_login_error.png')
            return False

    async def scrape_products(self, keyword: str, max_products: int = 10) -> List[Dict]:
        """
        ê²€ìƒ‰ì–´ë¡œ ìƒí’ˆì„ ì°¾ì•„ ìƒì„¸ ì •ë³´ê¹Œì§€ ëª¨ë‘ ìˆ˜ì§‘

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_products: ìˆ˜ì§‘í•  ìµœëŒ€ ìƒí’ˆ ê°œìˆ˜

        Returns:
            ìƒí’ˆ ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            start_time = time.time()

            # ë¸Œë¼ìš°ì € ì´ˆê¸°í™” (ë¡œê·¸ì¸ ì—†ì´)
            print("â±ï¸ [STEP 1/5] ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹œì‘...")
            logger.info("â±ï¸ [STEP 1/5] ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹œì‘...")
            step_start = time.time()
            await self._init_browser()
            print(f"âœ… [STEP 1/5] ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            logger.info(f"âœ… [STEP 1/5] ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")

            # 1ë‹¨ê³„: ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì†
            print("â±ï¸ [STEP 2/5] ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            logger.info("â±ï¸ [STEP 2/5] ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            step_start = time.time()
            search_url = f"https://kream.co.kr/search?keyword={keyword}&tab=products"

            await self.page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
            # ê²€ìƒ‰ í˜ì´ì§€ë„ React ì•±ì´ë¯€ë¡œ ì¶©ë¶„íˆ ëŒ€ê¸°
            await asyncio.sleep(5)
            print(f"âœ… [STEP 2/5] ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")
            logger.info(f"âœ… [STEP 2/5] ê²€ìƒ‰ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ)")

            # ë””ë²„ê·¸: ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            await self.page.screenshot(path='/tmp/kream_search_page.png')
            print("ğŸ“¸ ê²€ìƒ‰ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: /tmp/kream_search_page.png")
            logger.info("ğŸ“¸ ê²€ìƒ‰ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: /tmp/kream_search_page.png")

            # í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ìƒí’ˆ ë¡œë“œ
            print("â±ï¸ [STEP 3/5] í˜ì´ì§€ ìŠ¤í¬ë¡¤ ë° ìƒí’ˆ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
            logger.info("â±ï¸ [STEP 3/5] í˜ì´ì§€ ìŠ¤í¬ë¡¤ ë° ìƒí’ˆ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
            step_start = time.time()
            for i in range(2):
                await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                await asyncio.sleep(1.0)

            # product_id ì¶”ì¶œ
            product_links = await self.page.query_selector_all("a[href*='/products/']")
            print(f"ğŸ” ì°¾ì€ ë§í¬ ìˆ˜: {len(product_links)}ê°œ")
            logger.info(f"ğŸ” ì°¾ì€ ë§í¬ ìˆ˜: {len(product_links)}ê°œ")
            product_ids = []

            for link in product_links[:max_products * 3]:  # ì—¬ìœ ìˆê²Œ ìˆ˜ì§‘
                href = await link.get_attribute('href')
                if href and '/products/' in href:
                    try:
                        product_id = href.split('/products/')[1].split('?')[0]
                        if product_id and product_id.isdigit() and product_id not in product_ids:
                            product_ids.append(product_id)
                            if len(product_ids) >= max_products:
                                break
                    except:
                        continue

            if not product_ids:
                logger.warning(f"ê²€ìƒ‰ì–´ '{keyword}'ì— ëŒ€í•œ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                await self.page.screenshot(path='/tmp/kream_no_products.png')
                return []

            logger.info(f"âœ… [STEP 3/5] ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ) - {len(product_ids)}ê°œ ìƒí’ˆ")

            # 4ë‹¨ê³„: ê° product_idì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ì¡°íšŒ
            logger.info(f"â±ï¸ [STEP 4/5] ìƒí’ˆ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ({len(product_ids)}ê°œ)...")
            step_start = time.time()
            products = []
            for i, product_id in enumerate(product_ids, 1):
                product_start = time.time()
                logger.info(f"  â±ï¸ [{i}/{len(product_ids)}] ìƒí’ˆ {product_id} ì²˜ë¦¬ ì¤‘...")

                product_data = await self._get_product_info(product_id)

                if product_data:
                    products.append(product_data)
                    logger.info(f"  âœ… [{i}/{len(product_ids)}] ì™„ë£Œ ({time.time() - product_start:.2f}ì´ˆ)")
                else:
                    logger.warning(f"  âš ï¸ [{i}/{len(product_ids)}] ì‹¤íŒ¨ ({time.time() - product_start:.2f}ì´ˆ)")

                # ì„œë²„ ë¶€í•˜ ë°©ì§€ ë° ë´‡ ì°¨ë‹¨ íšŒí”¼ (2-3ì´ˆ ëœë¤ ëŒ€ê¸°)
                wait_time = random.uniform(2.0, 3.0)
                logger.info(f"  â³ ë‹¤ìŒ ìƒí’ˆê¹Œì§€ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                await asyncio.sleep(wait_time)

            logger.info(f"âœ… [STEP 4/5] ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ({time.time() - step_start:.2f}ì´ˆ) - {len(products)}ê°œ ì„±ê³µ")

            logger.info(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            return products

        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return []

        finally:
            # ë¸Œë¼ìš°ì € ì •ë¦¬
            await self.close()

    async def _get_product_info(self, product_id: str) -> Optional[Dict]:
        """ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            product_url = f"https://kream.co.kr/products/{product_id}"

            await self.page.goto(product_url, wait_until='domcontentloaded', timeout=60000)
            await asyncio.sleep(2)

            # í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸° - ë” ì˜¤ë˜ ëŒ€ê¸°
            await asyncio.sleep(5)
            content = await self.page.content()

            # ë””ë²„ê·¸: HTML ì €ì¥
            with open(f'/tmp/kream_product_{product_id}.html', 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"HTML ì €ì¥ë¨: /tmp/kream_product_{product_id}.html (í¬ê¸°: {len(content)} bytes)")

            soup = BeautifulSoup(content, 'html.parser')

            # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
            product_data = {
                'product_id': product_id,
                'product_name_ko': '',
                'product_name_en': '',
                'model_number': '',
                'brand': '',
                'color': '',
                'release_price': None,
                'image_url': None,
                'source_url': product_url,
            }

            # 1. JSON-LD ìŠ¤í‚¤ë§ˆì—ì„œ ì •ë³´ ì¶”ì¶œ (ê°€ì¥ ì •í™•í•¨)
            json_ld_script = soup.find("script", {"type": "application/ld+json", "id": "Product"})
            if json_ld_script:
                try:
                    json_data = json.loads(json_ld_script.string)
                    logger.info(f"JSON-LD ë°ì´í„° ë°œê²¬: {json_data.get('name')}")

                    # ì˜ë¬¸ ìƒí’ˆëª…
                    if json_data.get("name"):
                        product_data["product_name_en"] = json_data["name"]

                    # ë¸Œëœë“œ
                    if json_data.get("brand") and json_data["brand"].get("name"):
                        product_data["brand"] = json_data["brand"]["name"]

                    # ëª¨ë¸ë²ˆí˜¸ (SKU)
                    if json_data.get("sku"):
                        product_data["model_number"] = json_data["sku"]

                    # ì´ë¯¸ì§€ URL
                    if json_data.get("image") and len(json_data["image"]) > 0:
                        product_data["image_url"] = json_data["image"][0]

                    # ê°€ê²© (í˜„ì¬ê°€ - ë°œë§¤ê°€ ì•„ë‹˜, ì¼ë‹¨ ë³´ë¥˜)
                    # if json_data.get("offers") and json_data["offers"].get("price"):
                    #     try:
                    #         product_data["release_price"] = int(json_data["offers"]["price"])
                    #     except:
                    #         pass

                    # descriptionì—ì„œ í•œê¸€ ìƒí’ˆëª… ì¶”ì¶œ
                    if json_data.get("description"):
                        desc = json_data["description"]
                        parts = desc.split(" ")
                        korean_parts = [p for p in parts if any("ê°€" <= c <= "í£" for c in p)]
                        if korean_parts and len(korean_parts) > 1:
                            product_data["product_name_ko"] = " ".join(korean_parts[:10]).split("ìƒí’ˆì„")[0].strip()

                    # ìƒ‰ìƒ ì¶”ì¶œ (ì˜ë¬¸ ìƒí’ˆëª…ì—ì„œ)
                    if json_data.get("name"):
                        product_data["color"] = self._extract_color_from_name(json_data["name"])
                except Exception as e:
                    logger.error(f"JSON-LD íŒŒì‹± ì‹¤íŒ¨: {e}")

            # 4. Meta íƒœê·¸ì—ì„œ ì •ë³´ ë³´ì™„
            if not product_data["product_name_ko"]:
                title_tag = soup.find("title")
                if title_tag:
                    title_text = title_tag.get_text()
                    if "|" in title_text:
                        product_data["product_name_ko"] = title_text.split("|")[0].strip()
            
            if not product_data["model_number"]:
                keywords_meta = soup.find("meta", {"name": "keywords"})
                if keywords_meta and keywords_meta.get("content"):
                    keywords = keywords_meta["content"]
                    if "," in keywords:
                        first_keyword = keywords.split(",")[0].strip()
                        if re.match(r"^[A-Z0-9\-]+$", first_keyword):
                            product_data["model_number"] = first_keyword
            
            if not product_data["brand"]:
                brand_meta = soup.find("meta", {"name": "product:brand"})
                if brand_meta and brand_meta.get("content"):
                    product_data["brand"] = brand_meta["content"]


            # ë””ë²„ê·¸: ì¶”ì¶œëœ ë°ì´í„° ë¡œê¹…
            logger.info(f"[DEBUG] ì¶”ì¶œëœ ìƒí’ˆ ì •ë³´ (ID: {product_id}):")
            for key, value in product_data.items():
                if value:
                    logger.info(f"  - {key}: {value}")
            
            if not product_data.get('product_name_ko') and not product_data.get('brand'):
                logger.warning(f"âš ï¸ ìƒí’ˆ ì •ë³´ê°€ ë¹„ì–´ìˆìŒ (ID: {product_id}). HTML íŒŒì¼ í™•ì¸ í•„ìš”.")
            else:
                logger.info(f"âœ… ìƒí’ˆ ì •ë³´ ì¶”ì¶œ: {product_data.get('product_name_ko') or product_data.get('brand')} (ID: {product_id})")
            
            return product_data

        except Exception as e:
            logger.error(f"âŒ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ (ID: {product_id}): {e}")
            return None

    def _extract_color_from_name(self, name: str) -> str:
        """ìƒí’ˆëª…ì—ì„œ ìƒ‰ìƒ ì¶”ì¶œ"""
        # ì¼ë°˜ì ì¸ ì‹ ë°œ ìƒ‰ìƒ íŒ¨í„´
        color_patterns = [
            r'\b(Black|White|Red|Blue|Green|Yellow|Orange|Purple|Pink|Brown|Grey|Gray|Navy|Beige|Cream|Tan|Olive|Maroon)\b',
            r'\b(ë¸”ë™|í™”ì´íŠ¸|ë ˆë“œ|ë¸”ë£¨|ê·¸ë¦°|ì˜ë¡œìš°|ì˜¤ë Œì§€|í¼í”Œ|í•‘í¬|ë¸Œë¼ìš´|ê·¸ë ˆì´|ë„¤ì´ë¹„|ë² ì´ì§€|í¬ë¦¼|íƒ„|ì˜¬ë¦¬ë¸Œ|ë§ˆë£¬)\b',
        ]

        colors = []
        for pattern in color_patterns:
            matches = re.findall(pattern, name, re.IGNORECASE)
            colors.extend(matches)

        return " ".join(colors) if colors else ""

    async def download_image(self, image_url: str, brand_name: str, model_number: str) -> str:
        """
        ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë¡œì»¬ ì €ì¥

        Args:
            image_url: ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ URL
            brand_name: ë¸Œëœë“œëª… (í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©)
            model_number: ëª¨ë¸ë²ˆí˜¸ (íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©)

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)
        """
        try:
            # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
            base_path = Path("/app/uploads/products")
            brand_folder = base_path / brand_name.replace(" ", "_")
            brand_folder.mkdir(parents=True, exist_ok=True)

            # íŒŒì¼ëª…: ëª¨ë¸ë²ˆí˜¸.png
            filename = f"{model_number.replace('/', '_')}.png"
            file_path = brand_folder / filename

            # ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
            if file_path.exists():
                logger.info(f"âœ… ì´ë¯¸ì§€ ì´ë¯¸ ì¡´ì¬: {file_path}")
                return f"uploads/products/{brand_name.replace(' ', '_')}/{filename}"

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(image_url)
                response.raise_for_status()

                # íŒŒì¼ ì €ì¥
                with open(file_path, 'wb') as f:
                    f.write(response.content)

                logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_path}")
                return f"uploads/products/{brand_name.replace(' ', '_')}/{filename}"

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return image_url  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ë°˜í™˜

    async def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        logger.info("ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
